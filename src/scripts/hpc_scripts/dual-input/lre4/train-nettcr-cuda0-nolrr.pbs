#!/bin/bash -l

# Walltime: The maximum time a job can run before being stopped.
# Format is HH:MM:SS
#PBS -l walltime=23:00:00

# Nodes have 2 GPUs and 28 cores each. The following makes sure that
# no more than 2 jobs will ever land on the same node.
#PBS -L tasks=1:lprocs=14

# Specify GPU queue
#PBS -l advres=gpu-reservation.202

# Give your job a unique name
### PBS -N

# Make sure that the environment in which the job runs is the same as
# the environment in which it was submitted.
# Declares that all environment variables in the qsub commands
# environment are to be exported to the batch job.
#PBS -V

# redirect standard output (-o) and error (-e) (optional)
# if omitted, the name of the job (specified by -N) or
# a generic name (name of the script followed by .o or .e and
# job number) will be used
### PBS -o ../../../models/vsc_logs/$PBS_JOBNAME_$PBS_JOBID.log
### PBS -e ../../../models/vsc_logs/$PBS_JOBNAME_$PBS_JOBID.err

# send mail notification (optional)
#   a        when job is aborted
#   b        when job begins
#   e        when job ends
#   M        your e-mail address (should default to email used to register on VSC)
#PBS -m bea

# Using PBS - Environment Variables :
# When a batch job starts execution, a number of environment variables are
# predefined, which include:
#      Variables defined on the execution host.
#      Variables exported from the submission host with
#                -v (selected variables) and -V (all variables).
#      Variables defined by PBS.
#
# The following reflect the environment where the user ran qsub:
# PBS_O_HOST    The host where you ran the qsub command.
# PBS_O_LOGNAME Your user ID where you ran qsub.
# PBS_O_HOME    Your home directory where you ran qsub.
# PBS_O_WORKDIR The working directory where you ran qsub.
#
# These reflect the environment where the job is executing:
# PBS_ENVIRONMENT       Set to PBS_BATCH to indicate the job is a batch job,
#         or to PBS_INTERACTIVE to indicate the job is a PBS interactive job.
# PBS_O_QUEUE   The original queue you submitted to.
# PBS_QUEUE     The queue the job is executing from.
# PBS_JOBID     The job's PBS identifier.
# PBS_JOBNAME   The job's name.

conda activate deepTCR

# Go to the absolute path of the current working directory of the qsub command.
# This means the qsub command should be performed in the hpc_scripts directory, which is a sub-sub-directory of the project root
cd "$PBS_O_WORKDIR"

# Export the project root folder
# PROJECT_ROOT="$( cd "$( dirname "${BASH_SOURCE[0]}" )/../.." >/dev/null 2>&1 && pwd )"}
export PROJECT_ROOT=$(readlink --canonicalize ../../../../../) # old method
# use `$(cd ../../.. ; pwd)` if you don't want to resolve symlinks

# specify which GPU to run on
export CUDA_VISIBLE_DEVICES=0

# indicate the amount of GPUS to use
export GPUS=1

# start of logging
echo "Start of job: ${PBS_JOBNAME}"
date

# trbmhci 5repeated5fold

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhci-repeated5fold-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhci-repeated5fold-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhci-repeated5fold-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhci-repeated5fold-negref-nettcr-b32-lre3-lrr.log"

# down

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown-repeated5fold-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown-repeated5fold-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown-repeated5fold-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown-repeated5fold-negref-nettcr-b32-lre3-lrr.log"


python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down400.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown400-repeated5fold-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown400-repeated5fold-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down400.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown400-repeated5fold-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown400-repeated5fold-negref-nettcr-b32-lre3-lrr.log"

# decoy

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidecoy-repeated5fold-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidecoy-repeated5fold-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidecoy-repeated5fold-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidecoy-repeated5fold-negref-nettcr-b32-lre3-lrr.log"

# down decoy

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidowndecoy-repeated5fold-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidowndecoy-repeated5fold-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidowndecoy-repeated5fold-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidowndecoy-repeated5fold-negref-nettcr-b32-lre3-lrr.log"


python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down400-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown400decoy-repeated5fold-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown400decoy-repeated5fold-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv repeatedkfold \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down400-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown400decoy-repeated5fold-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown400decoy-repeated5fold-negref-nettcr-b32-lre3-lrr.log"

#################################

# epitope-grouped

# down

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv epitope_grouped \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown-epitope_grouped-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown-epitope_grouped-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv epitope_grouped \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidown-epitope_grouped-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidown-epitope_grouped-negref-nettcr-b32-lre3-lrr.log"

# down decoy

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv epitope_grouped \
--n_folds 5 \
--full_dataset_path "${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human.csv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidowndecoy-epitope_grouped-shuffle-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidowndecoy-epitope_grouped-shuffle-nettcr-b32-lre3-lrr.log"

python ${PROJECT_ROOT}/src/scripts/train/scenario_separated_inputs.py \
--batch_size 32 \
--epochs 20 \
--cv epitope_grouped \
--n_folds 5 \
--neg_ref "${PROJECT_ROOT}/data/raw/CDR3_control_sequences.tsv" \
--min_length_cdr3 10 \
--max_length_cdr3 20 \
--min_length_epitope 8 \
--max_length_epitope 11 \
--data_path ${PROJECT_ROOT}/data/interim/vdjdb-2019-08-08/vdjdb-human-trb-mhci-no10x-size-down-decoy.csv \
--model nettcr \
--learning_rate 0.0001 \
--optimizer rmsprop \
--name "trbmhcidowndecoy-epitope_grouped-negref-nettcr-b32-lre3-lrr" 2>&1 | tee -a "trbmhcidowndecoy-epitope_grouped-negref-nettcr-b32-lre3-lrr.log"
