#!/bin/bash -l


# Walltime: The maximum time a job can run before being stopped. 
# If not used a default of a few minutes is used. 
# Use this flag to prevent jobs that go bad running for hundreds of hours. 
# Format is HH:MM:SS
#PBS -l walltime=24:00:00

#PBS -L tasks=1:lprocs=all:place=node

# Specify GPU queue
#PBS -l advres=gpu-reservation.202


# Give your job a unique name
#PBS -N unnamed_job

# Make sure that the environment in which the job runs is the same as 
# the environment in which it was submitted.
#PBS -V

# redirect standard output (-o) and error (-e) (optional)
# if omitted, the name of the job (specified by -N) or
# a generic name (name of the script followed by .o or .e and 
# job number) will be used
#PBS -o output/vsclogs/$PBS_JOBNAME.$PBS_JOBID.log
#PBS -e output/vsclogs/$PBS_JOBNAME.$PBS_JOBID.err

# send mail notification (optional)
#   a        when job is aborted
#   b        when job begins
#   e        when job ends
#   M        your e-mail address (should always be specified)
#PBS -m ae

# Using PBS - Environment Variables :
#
# When a batch job starts execution, a number of environment variables are
# predefined, which include:
#      Variables defined on the execution host.
#      Variables exported from the submission host with
#                -v (selected variables) and -V (all variables).
#      Variables defined by PBS.
#
# The following reflect the environment where the user ran qsub:
# PBS_O_HOST    The host where you ran the qsub command.
# PBS_O_LOGNAME Your user ID where you ran qsub.
# PBS_O_HOME    Your home directory where you ran qsub.
# PBS_O_WORKDIR The working directory where you ran qsub.
#
# These reflect the environment where the job is executing:
# PBS_ENVIRONMENT       Set to PBS_BATCH to indicate the job is a batch job,
#         or to PBS_INTERACTIVE to indicate the job is a PBS interactive job.
# PBS_O_QUEUE   The original queue you submitted to.
# PBS_QUEUE     The queue the job is executing from.
# PBS_JOBID     The job's PBS identifier.
# PBS_JOBNAME   The job's name.


# First load the appropriate cluster module
# Then, activate the appropriate software packages
module load TensorFlow/1.12.0-intel-2018b-GPU-Python-3.6.8-Keras-2.2.4

# go to the (current) working directory (optional, if this is the
# directory where you submitted the job)
cd "$PBS_O_WORKDIR"

. env/bin/activate

echo Start Job
date

# suppress error when using multiprocessing
export HDF5_USE_FILE_LOCKING=FALSE

# indicate the amount of GPUS to use
export GPUS=2

# run the script

echo "Arg 1: $1"


if [ -z "$PBS_JOBNAME" ]
  then
    echo "Wrong usage. Supply a mode (padding, gap, ...) and a name for the job."
    exit 1
fi


case "$1" in
        padding)
            shift
            python scripts/scenario_padding.py run --name "$PBS_JOBNAME" "$@"
            ;;
        
        gap)
            shift
            python scripts/scenario_gap.py run --name "$PBS_JOBNAME" "$@"
            ;;
        
        dense)
            shift
            python scripts/scenario_dense.py run --name "$PBS_JOBNAME" "$@"
            ;;
        
        ppi)
            shift
            python scripts/scenario_ppi.py run --name "$PBS_JOBNAME" "$@"
            ;;
        
        nettcr)
            shift
            python scripts/scenario_nettcr.py run --name "$PBS_JOBNAME" "$@"
            ;;

        ppilit)
            shift
            python scripts/scenario_ppi_lit.py run --name "$PBS_JOBNAME" "$@"
            ;;
        
        ppilitvdjdb)
            shift
            python scripts/scenario_ppi_lit_vdjdb.py run --name "$PBS_JOBNAME" "$@"
            ;;

        *)
            echo "Wrong usage. Supply a mode (padding, gap, ...) and a name for the job."
            exit 1
 
esac

echo End Job

