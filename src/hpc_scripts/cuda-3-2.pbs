#!/bin/bash -l

# Walltime: The maximum time a job can run before being stopped.
# Format is HH:MM:SS
#PBS -l walltime=11:59:00

# Nodes have 2 GPUs and 28 cores each. The following makes sure that
# no more than 2 jobs will ever land on the same node.
#PBS -L tasks=1:lprocs=14

# Specify GPU queue
#PBS -l advres=gpu-reservation.202

# Give your job a unique name
### PBS -N cuda-0-padded-comparison

# Make sure that the environment in which the job runs is the same as
# the environment in which it was submitted.
# Declares that all environment variables in the qsub commands
# environment are to be exported to the batch job.
#PBS -V

# redirect standard output (-o) and error (-e) (optional)
# if omitted, the name of the job (specified by -N) or
# a generic name (name of the script followed by .o or .e and
# job number) will be used
### PBS -o ../../../models/vsc_logs/$PBS_JOBNAME_$PBS_JOBID.log
### PBS -e ../../../models/vsc_logs/$PBS_JOBNAME_$PBS_JOBID.err

# send mail notification (optional)
#   a        when job is aborted
#   b        when job begins
#   e        when job ends
#   M        your e-mail address (should default to email used to register on VSC)
#PBS -m bea

# Using PBS - Environment Variables :
# When a batch job starts execution, a number of environment variables are
# predefined, which include:
#      Variables defined on the execution host.
#      Variables exported from the submission host with
#                -v (selected variables) and -V (all variables).
#      Variables defined by PBS.
#
# The following reflect the environment where the user ran qsub:
# PBS_O_HOST    The host where you ran the qsub command.
# PBS_O_LOGNAME Your user ID where you ran qsub.
# PBS_O_HOME    Your home directory where you ran qsub.
# PBS_O_WORKDIR The working directory where you ran qsub.
#
# These reflect the environment where the job is executing:
# PBS_ENVIRONMENT       Set to PBS_BATCH to indicate the job is a batch job,
#         or to PBS_INTERACTIVE to indicate the job is a PBS interactive job.
# PBS_O_QUEUE   The original queue you submitted to.
# PBS_QUEUE     The queue the job is executing from.
# PBS_JOBID     The job's PBS identifier.
# PBS_JOBNAME   The job's name.


# First load the appropriate cluster module
# Then, activate the appropriate software packages
module load TensorFlow/1.12.0-intel-2018b-GPU-Python-3.6.8-Keras-2.2.4

# Go to the absolute path of the current working directory of the qsub command.
cd "$PBS_O_WORKDIR"

# Export the project root folder
# PROJECT_ROOT="$( cd "$( dirname "${BASH_SOURCE[0]}" )/../.." >/dev/null 2>&1 && pwd )"}
export PROJECT_ROOT=$(readlink --canonicalize ../../) # old method
# use `$(cd ../../.. ; pwd)` if you don't want to resolve symlinks

# activate virtualenv
# `.` is prefered over `source`
# https://stackoverflow.com/questions/11027782/virtualenv-venv-bin-activate-vs-source-venv-bin-activate
. ${PROJECT_ROOT}/deepTCR-env/bin/activate

# specify which GPU to run on
export CUDA_VISIBLE_DEVICES=0

# suppress error when using multiprocessing
export HDF5_USE_FILE_LOCKING=FALSE

# indicate the amount of GPUS to use
export GPUS=1

# start of logging
echo "Start of job: ${PBS_JOBNAME}"
date

# run the script
echo "Arg 1: $1"

# # human trb
# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-f10-absdif-human-trb --batch_size 512 --epochs 40 --nrFolds 10 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/vdjdb-human-trb.csv

# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b128-e40-f5-absdif-human-trb --batch_size 128 --epochs 40 --nrFolds 5 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/vdjdb-human-trb.csv

# # human trb-tra
# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-f10-absdif-human-ab --batch_size 512 --epochs 40 --nrFolds 10 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/vdjdb-human.csv

# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b128-e40-f5-absdif-human-ab --batch_size 128 --epochs 40 --nrFolds 5 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/vdjdb-human.csv

# # older dataset 30-10-18
python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-f10-absdif-all-301018 --batch_size 512 --epochs 40 --nrFolds 10 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_all_unique.csv

# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b128-e40-f5-absdif-all-301018 --batch_size 128 --epochs 40 --nrFolds 5 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_all_unique.csv

# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-f10-absdif-human-ab-301018 --batch_size 512 --epochs 40 --nrFolds 10 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_tra_trb_human_unique.csv

# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-f10-absdif-human-trb-301018 --batch_size 512 --epochs 40 --nrFolds 10 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_trb_human_unique.csv

# python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b128-e40-f5-absdif-human-trb-301018 --batch_size 128 --epochs 40 --nrFolds 5 --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_trb_human_unique.csv

# new data - stratified
python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-strat4-absdif-human-trb --batch_size 512 --epochs 40 --nrFolds 4 --stratified True --data_path ${PROJECT_ROOT}/data/interim/vdjdb-human-trb.csv

python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b128-e40-strat4-absdif-human-trb --batch_size 128 --epochs 40 --nrFolds 4 --stratified True --data_path ${PROJECT_ROOT}/data/interim/vdjdb-human-trb.csv

python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-strat4-absdif-human-ab --batch_size 512 --epochs 40 --nrFolds 4 --stratified True --data_path ${PROJECT_ROOT}/data/interim/vdjdb-human.csv

# older dataset 30-10-18 - stratified
#python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-strat4-absdif-all-301018 --batch_size 512 --epochs 40 --nrFolds 3 --stratified True --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_all_unique.csv

#python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-strat3-absdif-human-ab-301018 --batch_size 512 --epochs 40 --nrFolds 3 --stratified True --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_tra_trb_human_unique.csv

#python ${PROJECT_ROOT}/src/model_scripts/scenarioPadding.py run --name pad-b512-e40-strat3-absdif-human-trb-301018 --batch_size 512 --epochs 40 --nrFolds 3 --stratified True --operator absdiff --data_path ${PROJECT_ROOT}/data/interim/30-10-18-slim/vdjdb_trb_human_unique.csv

# trb-(unique) can take 4 folds
# all cannot
# human tra/trb cannot

# nettcr model


# no 10x data

echo "End of job ${PBS_JOBNAME}"
