This directory contains trained and evaluated models as defined by the different experiments in the bash and pbs scripts in `./src/scripts/hpc_scripts`. In the GitHub repository only two pre-trained models (used for new predictions as defined in the main README) are available, due to space constraints. The complete output (raw/processed datasets, trained model files in .h5 format, CV train/test splits, etc.) can be found in the Zenodo data repository: [10.5281/zenodo.3973547](https://doi.org/10.5281/zenodo.3973547).

**Log files for the experiments outlined here can be found in `./src/scripts/hpc_scripts`.**

- `models`: output directory where trained models will be stored automatically.

- `models-main`: contains the trained model directories for the different experiments described in the bash and pbs scripts in `./src/scripts/hpc_scripts`. Each directory was provided with metrics by running `./src/scripts/evaluate/visualize.py metrics .` inside of it (or alternatively, running `find . -maxdepth 1 -mindepth 1 -type d -exec python ../../../../../src/scripts/evaluate/visualize.py metrics --force True "{}" \;`, being careful to increase the number of `..` steps in case of a directory with deeper nesting).

- `comparisons`: contains comparison directories, each consisting of two or more model output directories, that contrast the performance metrics of the models. These outputs were generated by using the `./src/scripts/evaluate/visualize.py` script, or by using the oneliners in `./src/scripts/evaluate/visualise.sh`, which can operate on the entire comparisons directory at once.

- `models-full`: contains models that were trained on the complete VDJdb dataset without cross-validation, filtered on human TRB data, no 10x data and restricted to 10-20 (CDR3) or 8-11 (epitope) amino acid residues, with negatives that were generated by shuffling (i.e. sampling an negative epitope for each positive CDR3 sequence). One set of models uses downsampling to reduce the most abundant epitopes down to 400 pairs each, the other one does not use any downsampling. These models were also used for evaluating on the external Adaptive dataset, as outlined in `./src/scripts/evaluate/evaluate_adaptive.sh`, the TRA subset of sequences (`./src/scripts/evaluate/evaluate_tra.sh`), and decoy data (`./src/scripts/evaluate/evaluate_decoy_full.sh`).

- `models-decoyfit`: contains models that were trained on true data, but evaluated on data where epitopes were replaced by decoys.

- `models-repeat-local`: contains a number of repeated runs from `models-main`, used to estimate variability in model performance for multiple identical runs.

- `models-padded-epitoperatio`: contains a quick test of trained models (padded/interaction map) that use a different type of negative shuffling, see docstrings in `./src/processing/negative_sampler.py` for more info.

- `pretrained`: contains the two main full models (identical to `models-full`) in a cleaned up directory, serving as the default target for the `./src/scripts/predict/predict.py` script.

# Overview of different experiments

- Two main architectures were compared: the interaction map (or `padded`) CNN and a dual input CNN based on NetTCR (`nettcr`).
- Two different cross-validation strategies were used: a 5x repeated 5-fold CV (`repeated5fold`) and an epitope-grouped CV (`epitope_grouped`)
- The different dataset subsets are labelled as follows. Check the Makefile's `preprocess-vdjdb-aug-2019` command (and the underlying script `./src/scripts/preprocessing/preprocess_vdjdb.py`) for a more thorough overview of the different filtering options.
    - `mhci`: only MHCI class presented epitopes.
    - `trb`: only TRB CDR3 sequences.
    - `tra`: only TRA CDR3 sequences.
    - `tratrb`: both types of CDR3 sequences.
    - `down`: moderate downsampling of most abundant epitopes to 1000 pairs.
    - `down400`: strong downsampling of most abundant epitopes to 400 pairs.
    - `decoy`: decoy epitope data.
    - `reg001`: regularization factor 0.01 (only for padded/interaction type models, fixed value)
- Two different methods of generating negative TCR-epitope pairs were used: shuffling of positive pairs, i.e. sampling a single epitope from the positive pairs for each CDR3 sequence (`shuffle`), and sampling CDR3s from a reference repertoire (`negref`).
- The batch size is labelled as `b32` = a batch size of 32.
- The learning rate was always 0.0001 (`lre4`) or 0.001 (`lre3`).
